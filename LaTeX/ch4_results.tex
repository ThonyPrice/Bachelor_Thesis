\chapter{Results and analysis}

We performed classification with 4 different classifiers on 4 different datasets. In each dataset-classifier combination we measured classification accuracy when applying 4 different feature selection (FS) methods. The aim is to investigate the impact of feature selection, and the interaction between different classifiers and FS-methods. To build a basis for comparisons, classification accuracy was measured without FS too.

\section{Variation among factors of experiments}
\label{Variation_among_factors}

Before analyzing the results we consider potential interactions between different factors involved in the experiment, these are:

\begin{enumerate}
  \item Datasets
  \item Machine Learning classifiers
  \item Feature selection methods
\end{enumerate}

Having 4 of each factor, there are 64 distinct combinations. We are particular interested in two of the interactions, Datasets:FS-methods and Classifiers:FS-methods.

\subsection{FS-methods is dataset dependent}

Studying the interaction between datasets and FS-methods, we regarded these factors as independent variables (IVs). Each IV combination is represented by 4 values, the best accuracy achieved with current FS-method by each of the 4 classifiers. Plotting the mean values \ref{fig:comp_acc_datasets}, a positive correlation between all FS-methods and number of attributes in the dataset is evident. This indicates feature selection has a larger impact on classification accuracy when data has a higher dimensionality.

% ANOVA plots
\input{snippets/anova_plots}

To establish that FS-methods has a higher impact on datasets with more attributes 2-way analysis of variance (ANOVA) is performed. The results can be seen in \ref{table:anova_values_data}. The achieved p-value do not allow to conclude a significant difference of accuracy corresponding to dataset and FS-methods interaction. However, the p-value of Datasets allows to conclude accuracy is affected by dataset with a significance of 99.9\%.

% ANOVA computations, dataset-fs
\input{snippets/anova_dataset_fs}

These results indicates that independently of which FS-method in our scope that is applied, datasets with larger amount of features benefits more from feature selection when it comes to classification accuracy.

\subsection{FS-methods are classifier dependent}

We regarded datasets as an independent variable and compared classification accuracy between different classifiers and FS-methods. The plot in \ref{fig:comp_classif_datasets} visualizes the correlation between different classifiers and FS-methods...

Performing 2-way anova showed the interaction between classifier-fs combinations are significantly different with a p-value of X. The values are presented in table \ref{table:anova_values_classif}.

This means... It is particularly interesting because wrapper methods uses classifiers as a black box and should to our understanding be classifier independent.

\input{snippets/anova_classif_fs}


\section{Classification improvements}

After performing classification with and without available FS-methods we compared the classification accuracies in tables \ref{table:ANN}, \ref{table:NB}, \ref{table:SVM} and \ref{table:CART}.

The tables showed that predominantly classification accuracy is improved for all classifiers tested in our scope by use of \textit{some} feature selection method. Results presented in tables corresponding to ANN \ref{table:ANN}, NB \ref{table:NB} and SVM \ref{table:SVM} entails comparing the best performing FS-method to a full dataset consistently results in equal or improved accuracy. Only in DT \ref{table:CART} one dataset exists that performed better without feature selection.

However, majority of the classifiers had some FS-method that achieved lower accuracy compared to full dataset. In 11 of 16 instances at least one FS-method performed worse. This is in line with our findings that FS-method is both dataset and classifier dependent. Further it suggests one have to select the combination of classifier and FS-method carefully.

% --- Tables ---
\begin{table}[h]
  \input{../tables/ANN_tidy}
  \caption[]%
  {{\small Classification accuracy achieved by ANN was improved on all datasets by the use of some feature selection method.}}
  \label{table:ANN}
\end{table}

\begin{table}[h]
  \input{../tables/CART_tidy} \\
  \caption[]%
  {{\small All datasets except MIAS benefited from feature selection using CART Decision Tree classifier.}}
  \label{table:CART}
\end{table}

\begin{table}[h]
  \input{../tables/NB_tidy} \\
  \caption[]%
  {{\small Naive Bayes sees improvement or equivalent accuracy by feature selection on every dataset.}}
  \label{table:NB}
\end{table}

\begin{table}[h]
  \input{../tables/SVM_tidy} \\
  \caption[]%
  {{\small Classification accuracy achieved by SVM was improved or equivalent on every dataset with use of feature selection.}}
  \label{table:SVM}
\end{table}
% ---/Tables ---


\subsection{Differences among classifiers}

We found there is a large variance between results depending on which dataset, classifier and FS-method that is applied. These findings was addressed in section \ref{Variation_among_factors}. Here we further analyze these differences.

Each plot \ref{fig:plots_chi2}, \ref{fig:plots_entropy}, \ref{fig:plots_sbs} and \ref{fig:plots_sfs} represents one feature selection method. The subplots, like \ref{fig:RHH_chi2} visualises the classification accuracy on one dataset in respect to number of features.

\begin{table}[hp]
  \input{snippets/gain_comparison}
  \caption[]%
  {{\small Ranking of which classifiers gained most accuracy when comparing feature selection to full dataset.}}
  \label{table:gain_comparison}
\end{table}

\subsubsection{Artificial Neural Network}

Looking at the table \ref{table:gain_comparison} the accumulated gain was 1.34 which was the highest among all classifiers. However, ANN consistently performs the worst of all classifiers in terms of accuracy. The ANN also provides the least consistent results with strong fluctuations in the results and wide standard deviation margins. Such fluctuations may suggest issues regarding convergence. These characteristics are evident in plot \ref{fig:WBCD_chi2}.

\input{snippets/plots_chi2}

\subsubsection{Support Vector Machine}

The SVM behaves very differently in respect to each dataset. Improvements are seen with a larger subset of attributes in the EN and MIAS dataset as seen in \ref{fig:EN_chi2} and \ref{fig:MIAS_sfs}. A negative trend on accuracy is observed on the WBCD datasets which might indicate an issue with dimensionality. In \ref{fig:RHH_sbs} an improved accuracy is evident with maximal accuracy achieved on a subset suggesting a positive effect of feature selection.

\input{snippets/plots_entropy}

\subsubsection{Decision Tree}

The decision tree shows consistent performance generally increasing performance with an increased amount of features. Although in cases like plot \ref{fig:WBCD_chi2} and \ref{fig:MIAS_sbs} best accuracy is achieved with a subset of features displaying evident benefits of feature selection.

\input{snippets/plots_sbs}

\subsubsection{Naive Bayes}

Naive Bayes had the least accumulated gain from feature selection as seen in table \ref{table:gain_comparison}. The largest improvement was seen in plot \ref{fig:EN_chi2} using 2 of 4 available attributes. In plots \ref{fig:WBCD_entropy} and \ref{fig:RHH_entropy} the accuracy sees little to no improvement when increasing the number of attributes.

\input{snippets/plots_sfs}


\section{Computation time}

\textbf{Under construction, profiling not run yet.}

Profiling the execution of running all experiments X\% of CPU-time was allocated to the Wrapper algorithms. As mentioned a finding the best possible subset of features is considered a NP-hard problem meaning a solution can not be found in polynomial time. This clearly suggest favouring filtering methods when choosing a feature selection method having limited computational resources.

\section{Source of errors}
\label{sec:source_of_errors}

There are two main factor that may risk propagate error into the results, libraries and datasets.

The libraries provide all functionality of the classifiers and the filter selection methods, thus a major part of the implementation. A implementation built upon faulty methods can not provide any significant results. We use two libraries. The Scikit library \parencite{scikit-learn} is well renowned widely used in both industry and research thus inspire confidence in its robustness. The mlxtend library is used for SBS and SFS methods and is still a open source with less coverage than Scikit \parencite{mlextend}. Still it has a active community and many release versions indicating it's well managed.

Datasets... \textbf{Under construction...}

% Source of error can be expanded here, discuss effect of binary classification and chance of subsets containing only one class. This is addressed with stratifiedKfold but in data poor sets the problem can still remain
