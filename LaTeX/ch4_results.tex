\chapter{Results and Analysis}

We performed classification with four different classifiers on four different datasets. In each dataset-classifier combination we measured classification accuracy when applying four different feature selection (FS) methods. The aim is to investigate the impact of feature selection on classification accuracy, and the interaction between different classifiers and FS-methods. To enable a basis for comparisons, classification accuracy was measured without FS too.

\section{Impact of Factors: Datasets, Classifiers and FS-methods}
\label{Variation_among_factors}

Before analyzing the results we consider potential interactions between different factors involved in the experiment, these are:

\begin{enumerate}
  \item Datasets
  \item Machine Learning classifiers
  \item Feature selection methods
\end{enumerate}

Having four of each factor, there are 16 distinct combinations of classifiers and FS-methods with four measurements, one on each dataset. All datasets contain different information and therefore introduce necessary variance to the experiments. The variance allows us to suspect these results generalizes well onto other datasets in the domain of breast cancer, which are not included in this study. In our scope we are particular interested in the interaction of classifiers and FS-methods.

% \input{snippets/anova_plots}

\subsection{Evaluation of FS-methods and Classifiers}
\label{sec:fs_methods_classifiers}

To investigate the effect of feature selection on different classifiers, we performed two-way ANOVA. The values constitutes of four measurements by each of the 16 distinct classifier and FS-method combination. The four measurements come from each of the four datasets. Each measurement is the best accuracy achieved with a attribute subset of the dataset. The ANOVA result is presented in table \ref{table:anova_values_classif}.

\input{snippets/anova_classif_fs}

The **-significance of Classifier concludes that the selection of classifier effect what accuracy is achieved. The variance of the each respective classifier is visualized in figure \ref{fig:box_plots}. The box-plot entail that ANN on average perform worse than other classifiers. NB has the largest variance, with accuracy ranging from roughly 50\% to 100\%. CART performs the best, with the highest average accuracy.

The ANOVA result of Method conclude the selection of FS-method do not result in a significant difference in achieved accuracy. In figure \ref{fig:box_methods} all methods manifests a large variance but roughly in the same range, which emphasizes the result of the ANOVA. That is, there is no significant difference in accuracy when applying different FS-methods.

Analyzing these results together, first they suggest that the selection of classifier impacts the expected accuracy. Secondly, which FS-method is used together with the classifier do not effect expected accuracy. Lastly, no unique combination of classifier and FS-method are statistically superior to others.

It's important to note these measurements only include classification with FS-selection, they do not enable any conclusions regarding the impact of using, versus not using feature selection. Therefore, that is the next point of interest.

\input{snippets/boxplots}

\FloatBarrier
\section{Classification Improvements}

After performing classification with and without FS-methods, all results of each classifier was collected. Results are presented in tables for ANN \ref{table:ANN}, CART \ref{table:CART}, NB \ref{table:NB} and SVM \ref{table:SVM} where the highest achieved accuracy is highlighted in bold format. The improvement is measured in gain, the ratio between best achieved FS accuracy and full dataset accuracy.

% --- Tables ---
\input{snippets/combines_tables}

\FloatBarrier
\subsection{Classification Improvements' Significance}
\label{sec:Investigation_improvement}

To study the effect of feature selection on classification accuracy we perform a t-test. The t-test values constitutes of the accuracies achieved without feature selection as one distribution. The other are the best accuracies achieved with feature selection. The t-test values are summarized in \ref{table:t_test_values}.

\input{snippets/t_test_values}

Comparing the t-test results of all classifiers, t-stat value suggests a 28\% increased performance when applying feature selection, see table \ref{table:ttest_result}. However, significance is insufficient to reject the null hypothesis that distributions are equal. Consequently we can not conclude feature selection significantly improves classification accuracy based on our data.

To further analyze the differences, t-test is performed to compare each accuracy by each individual classifier, with and without feature selection. The tests concluded a significant improvement of accuracy using FS on ANN. No significant increase nor decrease in accuracy could be proven for the other classifiers; DT, CART and SVM.

\input{snippets/table_ttest}

The absence in the significance among the t-test results may be a consequence of data shortage. However, t-stat indicates some differences among classifiers that we'll analyze further.

\FloatBarrier
\subsection{Differences among Classifiers}

We found in \ref{sec:fs_methods_classifiers} that the selection of classifier affect the expected accuracy. In \ref{sec:Investigation_improvement} t-test scores concluded benefit of feature selection varies between different classifiers. Ranking the accumulated gain of each classifier from tables \ref{table:combines_tables}, we construct table \ref{table:gain_comparison} which clarifies variation of improved accuracy between classifiers. We'll now look at each classifier in turn:

\begin{table}[hp]
  \input{snippets/gain_comparison}
  \caption[]%
  {{\small Ranking of which classifiers gained most accuracy when comparing feature selection to full dataset.}}
  \label{table:gain_comparison}
\end{table}

\subsubsection{Artificial Neural Network}

Looking at the table \ref{table:gain_comparison} the accumulated gain was 1.2 which was the highest among all classifiers. However, ANN consistently performs the worst of all classifiers in terms of accuracy. The ANN also provides the least consistent results with strong fluctuations in the results and wide standard deviation margins. Such fluctuations may suggest issues regarding convergence as an effect of using default parameters in the network. These characteristics are evident in plot \ref{fig:ANN_WBCD} showcasing the accuracy by each FS-method as a function of number of attributes. However, t-test show an significant increase in classification accuracy when feature selection is applied.

\subsubsection{Support Vector Machine}

SVM improves accuracy in two out of four datasets in table \ref{table:SVM}. In these datasets the best performance is achieved by using wrapper methods.

The SVM behaves differently in respect to each dataset which may conclude different kernels of the SVM is needed on different datasets. Improvements are seen with a larger subset of attributes in the EN dataset \ref{fig:SVM_EN}. A negative trend on accuracy is observed on the WBCD dataset \ref{fig:SVM_WBCD} which might indicate an issue with dimensionality. In \ref{fig:SVM_RHH} an improved accuracy is evident with maximal accuracy achieved on a subset suggesting a positive effect of feature selection.

\subsubsection{Decision Tree}

The decision tree shows consistent performance, generally increasing accuracy with an increased amount of features. Although in cases like plot \ref{fig:CART_MIAS} and \ref{fig:CART_EN} best accuracy is achieved with a subset of features displaying evident benefits of feature selection.

\subsubsection{Na√Øve Bayes}

In plot \ref{fig:NB_WBCD} the accuracy presents little to no improvement when increasing the number of attributes. In three out of four datasets NB produces the highest accuracy with very few attributes indicating benefits of feature selection.

\input{snippets/plots_ANN}
\input{snippets/plots_CART}
\input{snippets/plots_NB}
\input{snippets/plots_SVM}

\newpage
\section{Computation time}
\label{sec:cumtime}

Profiling the execution of running all experiments approximately 100\% of CPU-time was allocated to the Wrapper algorithms as showed in table \ref{table:cpu}. Finding the best possible subset of features is considered a NP-hard problem meaning a solution can not be found in polynomial time. This clearly suggest favouring filtering methods when choosing a feature selection method having limited computational resources.

\input{snippets/cpu_table}
