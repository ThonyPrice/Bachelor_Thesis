\chapter{Results and analysis}

We performed classification with four different classifiers on four different datasets. In each dataset-classifier combination we measured classification accuracy when applying four different feature selection (FS) methods. The aim is to investigate the impact of feature selection, and the interaction between different classifiers and FS-methods. To build a basis for comparisons, classification accuracy was measured without FS too.

\section{Variation among factors of experiments}
\label{Variation_among_factors}

Before analyzing the results we consider potential interactions between different factors involved in the experiment, these are:

\begin{enumerate}
  \item Datasets
  \item Machine Learning classifiers
  \item Feature selection methods
\end{enumerate}

Having four of each factor, there are 64 distinct combinations. We are particular interested in two of the interactions, Datasets:FS-methods and Classifiers:FS-methods.

\subsection{Datasets with larger dimensionality achieves better classification accuracy}

Studying the interaction between datasets and FS-methods we regarded these factors as independent variables (IVs). Each IV combination is represented by four values, the best accuracy achieved with current FS-method by each of the four classifiers.

Two-way analysis of variance (ANOVA) is performed on the IVs. The results are presented in \ref{table:anova_values_data} and suggests three conclusions:

% ANOVA computations, dataset-fs
\input{snippets/anova_dataset_fs}

First, the significance of Dataset allows to conclude a dependency between accuracies and datasets is present. This is to be expected because different datasets provide varying basis for prediction. However, as all accuracies used in the test was obtained by feature selection this test gives no information on whether the correlation lies among the datasets themselves of the application of feature selection.

Secondly, the absence of significance in Method mean we can not reject that there is no correlation between FS-method and accuracy.

Thirdly, no significance of Dataset:Method indicates we can not reliably conclude that the interaction between datasets and methods has a correlation with achieved accuracy.

Visualizing the values used in the ANOVA test, the correlation between datasets and accuracy is apparent \ref{fig:comp_acc_datasets}. It is positive in respect to number of attributes suggesting either feature selection has a larger impact on classification accuracy when data has a higher dimensionality or higher dimensioned datasets on their own lead to better accuracy.

\input{snippets/anova_plots}

\subsection{Interaction between FS-methods and classifiers}
\label{sec:fs_methods_classifiers}

To investigate the effect of feature selection on different classifiers we performed two-way ANOVA. The values constitutes of each accuracy achieved of all distinct classifier and FS-method on all datasets. The ANOVA result is presented in \ref{table:anova_values_classif}.

\input{snippets/anova_classif_fs}

The significance of both Classifier and Method concludes they independently effect what accuracy is achieved. Their interaction, Classifier:Method is significant too and allows to reason there is a dependency between accuracy and the interaction between classifiers and methods. Plotting the values used in the test visualizes this correlation \ref{fig:comp_classif_datasets}.

To further analyze the interaction of classifiers and FS-methods we performed post-hoc analysis by Tukey's test \parencite{Haynes2013}. The significant results \ref{table:Tukey_values} show the difference among the groups lies mainly in which classifier is used.


The result indicates that some FS methods has a superior classification accuracy. 


\input{snippets/table_t_test_results}


To conclude these results, they suggests one primarily has to carefully select the classifier to use for classification. When choosing SVM, the choice of feature selection has greatest impact where wrapper methods perform better.


\section{Classification improvements}

After performing classification with and without FS-methods, all results of each classifier was collected. Results are presented in tables for ANN \ref{table:ANN}, CART \ref{table:CART}, NB \ref{table:NB} and SVM \ref{table:SVM} where the highest achieved accuracy is highlighted in bold format. The improvement is measured in gain, it is the ratio between best achieved FS accuracy and full dataset accuracy. From this we can conclude that classification accuracy is equal or improved for all classifiers on all datasets tested in our scope by use of \textit{some} feature selection method. The only exception is the Mias dataset with Decision tree \ref{table:CART}, it performed worse with feature selection.

However, majority of the classifiers has some FS-method that achieved lower accuracy compared to full dataset. Again this is viewed in tables \ref{table:ANN}, \ref{table:CART}, \ref{table:NB} and \ref{table:SVM}. In 11 of 16 instances at least one FS-method performed worse. This is in line with our findings that FS-method is classifier dependent suggesting one have to select the combination of classifier and FS-method with care.

% --- Tables ---
\input{snippets/combines_tables}

\subsection{Improvement is significant}

To ensure the suggestion that application of some FS-method improves classification accuracy t-test was performed. The t-test values constitutes of the accuracies achieved without feature selection as one distribution. The other are the best accuracies achieved with feature selection. The t-test values are summarized in \ref{table:t_test_values}.

\input{snippets/t_test_values}


Performing the t-test resulting a significant distinction between the distributions. This allows us to reject the null hypothesis that the distributions are equal with a confidence level of 90\%. Thus, improvement of classification accuracy by feature selection is statistically significant with an certainty of 90\%.

\subsection{Differences among classifiers}

As addressed in section \ref{Variation_among_factors} classifiers behave differently depending on FS-method and datasets.
In order to compare classification improvement among different FS-methods and datasets \textbf{Gain} is computed. Gain measures the improvement of FS as the ratio between best achieved FS accuracy and full dataset accuracy. The accumulated gain for each classifier is presented in \ref{table:gain_comparison}. Below these results and differences are analyzed in further detail with respect to each classifier.

\begin{table}[hp]
  \input{snippets/gain_comparison}
  \caption[]%
  {{\small Ranking of which classifiers gained most accuracy when comparing feature selection to full dataset.}}
  \label{table:gain_comparison}
\end{table}

\subsubsection{Artificial Neural Network}

Looking at the table \ref{table:gain_comparison} the accumulated gain was 1.34 which was the highest among all classifiers. However, ANN consistently performs the worst of all classifiers in terms of accuracy. The ANN also provides the least consistent results with strong fluctuations in the results and wide standard deviation margins. Such fluctuations may suggest issues regarding convergence. These characteristics are evident in plot \ref{fig:WBCD_chi2} showcasing the accuracy by each FS-method as a function of number of attributes..

\subsubsection{Support Vector Machine}

The SVM behaves very differently in respect to each dataset. Improvements are seen with a larger subset of attributes in the EN and MIAS dataset as seen in \ref{fig:EN_chi2} and \ref{fig:MIAS_sfs}. A negative trend on accuracy is observed on the WBCD datasets which might indicate an issue with dimensionality. In \ref{fig:RHH_sbs} an improved accuracy is evident with maximal accuracy achieved on a subset suggesting a positive effect of feature selection.

\subsubsection{Decision Tree}

The decision tree shows consistent performance generally increasing performance with an increased amount of features. Although in cases like plot \ref{fig:WBCD_chi2} and \ref{fig:MIAS_sbs} best accuracy is achieved with a subset of features displaying evident benefits of feature selection.

\subsubsection{Na√Øve Bayes}

Naive Bayes had the least accumulated gain from feature selection as seen in table \ref{table:gain_comparison}. The largest improvement was seen in plot \ref{fig:EN_chi2} using 2 of 4 available attributes. In plots \ref{fig:WBCD_entropy} and \ref{fig:RHH_entropy} the accuracy presents little to no improvement when increasing the number of attributes.

\input{snippets/plots_chi2}
\input{snippets/plots_entropy}
\input{snippets/plots_sbs}
\input{snippets/plots_sfs}

\newpage
\section{Computation time}

\textbf{Under construction, profiling not run yet.}

Profiling the execution of running all experiments X\% of CPU-time was allocated to the Wrapper algorithms. As mentioned a finding the best possible subset of features is considered a NP-hard problem meaning a solution can not be found in polynomial time. This clearly suggest favouring filtering methods when choosing a feature selection method having limited computational resources.

\section{Source of errors}
\label{sec:source_of_errors}

There are two main factor that may risk propagate error into the results, libraries and datasets.

The libraries provide all functionality of the classifiers, filter selection methods and analysis tools. A major part of the implementation therefore constitutes of these libraries. A implementation built upon faulty methods can not provide any trustworthy results. The Scikit library \parencite{scikit-learn} is well renowned and widely used in both industry and research thus inspire confidence in its robustness. The mlxtend library is used for SBS and SFS methods and is still a open source with less coverage than Scikit \parencite{mlextend}. Still it has a active community and many release versions indicating it's well managed.

Because we use a multitude of datasets their differences may influence our results in a way that invalidates our findings. \textcite{c201416} claim that feature selection methods can only be compared on the same dataset. However our comparisons do not regard the performance of specific FS-methods but their impact at large. Also, these interactions is addressed in \ref{Variation_among_factors}.

Due to varying number of examples of the datasets where some are rather small, evaluation by kFold may infer very small test batches and lead to skewed results. The impact may be enlarged considering our classification is binary which means consistently predicting one class where that class is predominant archives non representable results for such predictor. This influence is minimized by using stratified kFold but as no check of distribution of classes in test batches was made this might be a source of error.

In analysis of the results two-way anova is performed to evaluate the effect of two factors on accuracy. As mentioned in \ref{Variation_among_factors} our experiments conclude of three factors leading us to regard one of these as independent to the accuracy. If this is not the case the results from analysis may suffer from bias of this parameter.
