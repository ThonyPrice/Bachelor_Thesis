\chapter{Results and analysis}

The outline of the analysis address the overall effect of feature selection. Then, examination of each FS-method in respect to chosen classifier. Lastly, interaction between dataset characteristics and specific FS-methods are being presented. All analysis stems from results presented alongside.

Results are presented both in form of tables and plots. The tables showcases the best performance achieved by each feature selection method in comparison to using full dataset, that is, utilising all features. The plots illustrates accuracy of each feature selection method in respect to number of attributes used. Plots are organised by FS-method.


\section{Classification improvements}

\subsection{Feature selection generally improves accuracy}

Predominantly classification accuracy is improved for all classifiers tested in our scope by use of \textit{some} feature selection method. Results presented in tables corresponding to ANN \ref{table:ANN}, NB \ref{table:NB} and SVM \ref{table:SVM} entails comparing the best performing FS-method to a full dataset consistently results in equal or improved accuracy. Only in DT \ref{table:CART} one dataset exists that performed better without feature selection.

However, majority of the classifiers had some FS-method that achieved lower accuracy compared to full dataset. In 11 of 16 instances at least one FS-method performed worse. This suggests one have to select the combination of classifier and FS-method carefully. As no such combination proved consistently superior across all dataset the optimal classifier FS-combination may also depend on the data it is applied to.

% --- Tables ---
\begin{table}[htbp!]
  \input{../tables/ANN_tidy}
  \caption[]%
  {{\small Classification accuracy achieved by ANN was improved on all datasets by the use of some feature selection method.}}
  \label{table:ANN}
\end{table}

\begin{table}[htbp!]
  \input{../tables/CART_tidy} \\
  \caption[]%
  {{\small All datasets except MIAS benefited from feature selection using CART Decision Tree classifier.}}
  \label{table:CART}
\end{table}

\begin{table}[htbp!]
  \input{../tables/NB_tidy} \\
  \caption[]%
  {{\small Naive Bayes sees improvement or equivalent accuracy by feature selection on every dataset.}}
  \label{table:NB}
\end{table}

\begin{table}[htbp!]
  \input{../tables/SVM_tidy} \\
  \caption[]%
  {{\small Classification accuracy achieved by SVM was improved or equivalent on every dataset with use of feature selection.}}
  \label{table:SVM}
\end{table}
% ---/Tables ---


\subsection{Differences among datasets, classifiers and FS-methods}

There is a large variance between results depending on which dataset, classifier and FS-method that is applied. Dataset differences is addressed in section \ref{Variation_among_datasets}. Classifiers and FS methods variations presented in plots.

Each plot \ref{fig:plots_chi2}, \ref{fig:plots_entropy}, \ref{fig:plots_sbs} and \ref{fig:plots_sfs} represents one feature selection method. The subplots, like \ref{fig:RHH_chi2} visualises the classification accuracy on one dataset in respect to number of features.

\subsubsection{Classifiers}

ANN consistently performs the worst of all classifiers. The ANN also provides the least consistent results with strong fluctuations in the results as seen by the standard deviation margins. Such fluctuations may suggest issues regarding convergence.

The decision tree shows consistent performance generally increasing performance with an increased amount of features. Although in cases like plot \ref{fig:WBCD_chi2} and \ref{fig:MIAS_sbs} an best accuracy is achieved with a subset of features displaying evident benefits of feature selection.

% Naive Bayes

The SVM appears to behave very differently in respect to dataset. Improvements are seen with a larger subset of attributes in the EN and MIAS dataset as seen in \ref{fig:EN_chi2} and \ref{fig:MIAS_sfs}. A negative trend on accuracy is observed on the WBCD datasets which might indicate an issue with dimensionality. In \ref{fig:RHH_sbs} an improved accuracy is evident with maximal accuracy achieved on a subset suggesting a positive effect of feature selection.

\subsubsection{Feature selection methods}

Chi2 in combination reveals with cart reveals a loss in accuracy when using more than 23 attributes as seen in plot \ref{fig:WBCD_chi2}. It indicates a preference of more attributes on the RHH dataset in \ref{fig:RHH_chi2}.

\input{snippets/plots_chi2}

Feature selection by Entropy shows clear performance improvement on a subset compared to all features in \ref{fig:EN_entropy}. In plots \ref{fig:WBCD_entropy} and \ref{fig:RHH_entropy} the accuracy sees little to no improvement when increasing the number of attributes.

\input{snippets/plots_entropy}

Using the wrapper method SBS both NB and ANN has a decrease in accuracy using all features in the EN dataset \ref{fig:EN_sbs}. However, both CART and SVM seems to prefer all features for an higher accuracy. In \ref{fig:WBCD_sbs} SVM has an strong decrease in performance using increasingly more features. CART and NB has a slight improvement using more features up to 27 where they starts to drop in there accuracy rate. ANN has a very fluctuating scoring performance but has its highest accuracy using all features.  In the MIAS dataset using SBS shown in figure \ref{fig:MIAS_sbs} both SVM and NB prefers using all the features. ANN and CART has there highest performance using three features. In the plot \ref{fig:RHH_sbs} there seems to be an general trend that more features give an higher accuracy. CART drops slightly in accuracy at 9 or more features. NB drops in accuracy as well using the full dataset. ANN performs best using seven features.

\input{snippets/plots_sbs}

The figure \ref{fig:plots_sfs} using SFS have a similar result to figure \ref{fig:plots_sbs} using SBS. The main differences are in \ref{fig:WBCD_sfs} where the best result for ANN is using only 24 features instead of the full dataset and in figure \ref{fig:RHH_sfs} where the best result for ANN have four features instead of seven.

\input{snippets/plots_sfs}

\section{Variation among datasets}
\label{Variation_among_datasets}

The mean accuracy of each method with respect to the datasets shows a positive correlation, as visualised in plot \ref{fig:comp_acc_datasets}. The correlation indicates datasets with a larger amount of features benefit more from feature selection than datasets with smaller amount of features.

ANOVA shows a statistical significance in the correlation due to the found values of the test, table \ref{table:anova_values}. The significance is evident due to the very small value $P(>F)$, probability of not rejecting the null hypothesis. Thus we can confidently reject the null hypothesis which is datasets are significantly the same in respect to FS-methods.

% ANOVA plot
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.7\linewidth]{../plots_with_std_fill/comp_acc_datasets.png}
  \caption{Datasets and accuracy displays evident correlation independently of applied FS method.}
  \label{fig:comp_acc_datasets}
\end{figure}

% Table with ANOVA computations
\begin{table}[ht]
  \begin{center}
  \begin{tabular}{l|r|r|r|r|r|r|}
  \cline{2-7}
  & $RSS$ & $df$ & $F$ & $P(>F)$ & $\eta^2$ & $\omega^2$ \\ \cline{1-7}
  \multicolumn{1}{ |l| }{\textbf{Dataset}}
  & 0.1664 &  3.0 & 77.663 & 9.3990e-07 & 0.9191 & 0.9037 \\ \cline{1-7}
  \multicolumn{1}{ |l| }{\textbf{Method}}
  & 0.0082 &  3.0 & 3.8336 & 5.0899e-02 & 0.0454 & 0.0334 \\ \cline{1-7}
  \multicolumn{1}{ |l| }{\textbf{Residual}}
  & 0.0064 &  9.0 \\ \cline{1-3}
  \end{tabular}
  \caption{ANOVA computations show there is a very low probability there is no difference among datasets when applying FS methods}
  \label{table:anova_values}
  \end{center}
\end{table}


\section{Computation time}

Profiling the execution of running all experiments 80\% of CPU-time was allocated to the Wrapper algorithms. As mentioned a finding the best possible subset of features is considered a NP-hard problem meaning a solution can not be found in polynomial time. \textbf{Need to actually profile a run then expand these results.}

\section{Source of errors}
\label{sec:source_of_errors}

There are two main factor that may risk propagate error into the results, libraries and datasets.

The libraries provide all functionality of the classifiers and the filter selection methods, thus a major part of the implementation. A implementation built upon faulty methods can not provide any significant results. We use two libraries. The Scikit library \parencite{scikit-learn} is well renowned widely used in both industry and research thus inspire confidence in its robustness. The mlxtend library is used for SBS and SFS methods and is still a open source with less coverage than Scikit \parencite{mlextend}. Still it has a active community and many release versions indicating it's well managed.

% Source of error can be expanded here, discuss effect of binary classification and chance of subsets containing only one class. This is addressed with stratifiedKfold but in data poor sets the problem can still remain
