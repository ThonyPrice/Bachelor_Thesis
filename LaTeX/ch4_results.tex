\chapter{Results and analysis}

We performed classification with four different classifiers on four different datasets. In each dataset-classifier combination we measured classification accuracy when applying four different feature selection (FS) methods. The aim is to investigate the impact of feature selection, and the interaction between different classifiers and FS-methods. To build a basis for comparisons, classification accuracy was measured without FS too.

\section{Variation among factors of experiments}
\label{Variation_among_factors}

Before analyzing the results we consider potential interactions between different factors involved in the experiment, these are:

\begin{enumerate}
  \item Datasets
  \item Machine Learning classifiers
  \item Feature selection methods
\end{enumerate}

Having four of each factor, there are 64 distinct combinations. We are particular interested in two of the interactions, Datasets:FS-methods and Classifiers:FS-methods.

\subsection{FS-methods is dataset dependent}

Studying the interaction between datasets and FS-methods, we regarded these factors as independent variables (IVs). Each IV combination is represented by four values, the best accuracy achieved with current FS-method by each of the four classifiers. Plotting the mean values \ref{fig:comp_acc_datasets}, a positive correlation between all FS-methods and number of attributes in the dataset is evident. This indicates feature selection has a larger impact on classification accuracy when data has a higher dimensionality.

% ANOVA plots
\input{snippets/anova_plots}

To establish that FS-methods has a higher impact on datasets with more attributes two-way analysis of variance (ANOVA) is performed. The results can be seen in \ref{table:anova_values_data}. The achieved P-value do not allow to conclude a significant difference of accuracy corresponding to dataset and FS-methods interaction. However, the P-value of Datasets allows to conclude accuracy is affected by dataset with a significance of 99.9\%.

% ANOVA computations, dataset-fs
\input{snippets/anova_dataset_fs}

These results indicates that independently of which FS-method in our scope that is applied, datasets with larger amount of features benefits more from feature selection when it comes to classification accuracy.

\subsection{FS-methods are classifier dependent}
\label{sec:fs_methods_classifiers}

To investigate the effect of feature selection on different classifiers we are interested in the interactions of these variables. First we are interested in the characteristics of the interaction, secondly if these characteristics are significant.

First, we compare classification accuracy between different classifiers and FS-methods by plotting the mean accuracy over all datasets for each classifier-FS combination, \ref{fig:comp_classif_datasets}. The plot shows no clear correlation suggesting behavior of such combinations are independent.

Secondly, to conclude interaction between classifiers and FS-methods is independent we performed two-way ANOVA. The results can be viewed in table \ref{table:anova_values_classif}. Looking at the P-value for accuracy in relation to the interaction between Classifier:Method we can conclude that these interactions are independent with a confidence level of 95\%.

\input{snippets/anova_classif_fs}

These results suggests one has to carefully select the classifier and FS-method to achieve optimal results.


\section{Classification improvements}

After performing classification with and without FS-methods, all results of each classifier was collected. Results are presented in tables for ANN \ref{table:ANN}, CART \ref{table:CART}, NB \ref{table:NB} and SVM \ref{table:SVM} where the highest achieved accuracy is highlighted in bold format. From this we can conclude that classification accuracy is equal or improved for all classifiers on all datasets tested in our scope by use of \textit{some} feature selection method. The only exception is the Mias dataset with Decision tree \ref{table:CART}, it performed worse with feature selection.

However, majority of the classifiers has some FS-method that achieved lower accuracy compared to full dataset. Again this is viewed in tables \ref{table:ANN}, \ref{table:CART}, \ref{table:NB} and \ref{table:SVM}. In 11 of 16 instances at least one FS-method performed worse. This is in line with our findings that FS-method is classifier dependent suggesting one have to select the combination of classifier and FS-method with care.

% --- Tables ---
\begin{table}[h]
  \centering
  \input{../tables/ANN_tidy}
  \caption[]%
  {{\small Classification accuracy achieved by ANN. Accuracy improved on all datasets by the use of some feature selection method. Rows represent feature selection method, columns represent dataset, bold font indicates the highest value.}}
  \label{table:ANN}
\end{table}

\begin{table}[h]
  \centering
  \input{../tables/CART_tidy} \\
  \caption[]%
  {{\small All datasets except MIAS benefited in classification accuracy from feature selection using CART Decision Tree classifier. Rows represent feature selection method, columns represent dataset, bold font indicates the highest value.}}
  \label{table:CART}
\end{table}

\begin{table}[h]
  \centering
  \input{../tables/NB_tidy} \\
  \caption[]%
  {{\small Na\"ive Bayes sees improvement or equivalent accuracy by feature selection on every dataset. Rows represent feature selection method, columns represent dataset, bold font indicates the highest value.}}
  \label{table:NB}
\end{table}

\begin{table}[h]
  \centering
  \input{../tables/SVM_tidy} \\
  \caption[]%
  {{\small Classification accuracy achieved by SVM. Accuracy was improved or equivalent on every dataset with use of feature selection. Rows represent feature selection method, columns represent dataset, bold font indicates the highest value.}}
  \label{table:SVM}
\end{table}
% ---/Tables ---

\subsection{Improvement is significant}

To ensure the suggestion that application of some FS-method improves classification accuracy t-test was performed. Regarding the accuracies achieved without feature selection as one distribution and best accuracies achieved with feature selection as the other.

Performing the t-test the resulting P-value was 0.077. This allows us to reject the null hypothesis that the distributions are equal with a confidence level of 90\%. Thus proving improvement of classification accuracy by feature selection is statistically significant.

\subsection{Differences among classifiers}

As addressed in section \ref{Variation_among_factors} classifiers behave differently depending on FS-method and datasets.
In order to compare classification improvement among different FS-methods and datasets \textbf{Gain} is computed. Gain measures the improvement of FS as the ratio between best achieved FS accuracy and full dataset accuracy. The accumulated gain for each classifier is presented in \ref{table:gain_comparison}. Below these results and differences are analyzed in further detail with respect to each classifier.

\begin{table}[hp]
  \input{snippets/gain_comparison}
  \caption[]%
  {{\small Ranking of which classifiers gained most accuracy when comparing feature selection to full dataset.}}
  \label{table:gain_comparison}
\end{table}

\subsubsection{Artificial Neural Network}

Looking at the table \ref{table:gain_comparison} the accumulated gain was 1.34 which was the highest among all classifiers. However, ANN consistently performs the worst of all classifiers in terms of accuracy. The ANN also provides the least consistent results with strong fluctuations in the results and wide standard deviation margins. Such fluctuations may suggest issues regarding convergence. These characteristics are evident in plot \ref{fig:WBCD_chi2} showcasing the accuracy by each FS-method as a function of number of attributes..

\subsubsection{Support Vector Machine}

The SVM behaves very differently in respect to each dataset. Improvements are seen with a larger subset of attributes in the EN and MIAS dataset as seen in \ref{fig:EN_chi2} and \ref{fig:MIAS_sfs}. A negative trend on accuracy is observed on the WBCD datasets which might indicate an issue with dimensionality. In \ref{fig:RHH_sbs} an improved accuracy is evident with maximal accuracy achieved on a subset suggesting a positive effect of feature selection.

\subsubsection{Decision Tree}

The decision tree shows consistent performance generally increasing performance with an increased amount of features. Although in cases like plot \ref{fig:WBCD_chi2} and \ref{fig:MIAS_sbs} best accuracy is achieved with a subset of features displaying evident benefits of feature selection.

\subsubsection{Naive Bayes}

Naive Bayes had the least accumulated gain from feature selection as seen in table \ref{table:gain_comparison}. The largest improvement was seen in plot \ref{fig:EN_chi2} using 2 of 4 available attributes. In plots \ref{fig:WBCD_entropy} and \ref{fig:RHH_entropy} the accuracy sees little to no improvement when increasing the number of attributes.

\subsection{Combined plots of mean accuracy}
\input{snippets/plots_chi2}
\input{snippets/plots_entropy}
\input{snippets/plots_sbs}
\input{snippets/plots_sfs}


\section{Computation time}

\textbf{Under construction, profiling not run yet.}

Profiling the execution of running all experiments X\% of CPU-time was allocated to the Wrapper algorithms. As mentioned a finding the best possible subset of features is considered a NP-hard problem meaning a solution can not be found in polynomial time. This clearly suggest favouring filtering methods when choosing a feature selection method having limited computational resources.

\section{Source of errors}
\label{sec:source_of_errors}

There are two main factor that may risk propagate error into the results, libraries and datasets.

The libraries provide all functionality of the classifiers, filter selection methods and analysis tools. A major part of the implementation therefore constitutes of these libraries. A implementation built upon faulty methods can not provide any trustworthy results. The Scikit library \parencite{scikit-learn} is well renowned and widely used in both industry and research thus inspire confidence in its robustness. The mlxtend library is used for SBS and SFS methods and is still a open source with less coverage than Scikit \parencite{mlextend}. Still it has a active community and many release versions indicating it's well managed.

Because we use a multitude of datasets their differences may influence our results in a way that invalidates our findings. \textcite{c201416} claim that feature selection methods can only be compared on the same dataset. However our comparisons do not regard the performance of specific FS-methods but their impact at large. Also, these interactions is addressed in \ref{Variation_among_factors}.

Due to varying number of examples of the datasets where some are rather small, evaluation by kFold may infer very small test batches and lead to skewed results. The impact may be enlarged considering our classification is binary. This influence is minimized by using stratified kFold but as no check of distribution of classes in test batches was made this might be a source of error.
