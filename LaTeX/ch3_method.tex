\chapter{Method}

\section{Datasets}
\label{sec:Datasets}


\subsection{Wisconsin}

The dataset used in this thesis, Breast Cancer Wisconsin (Diagnostic) dataset, was donated 1995 to UCI  Machine Learning Repository \parencite{dua:2017} by one of its creators, Nick Street. It contains 569 instances with 30 attributes describing the features of breast cancer. Each instance is classified as benign (357) or malignant (212). The 32 attributes describe ten real-value features.

\subsection{Royal Hallamshire Hospital}

Fine needle aspirates of breast lumps (FNAB) was collected from 692 patients at Royal Hallamshire Hospital, Sheffield, during 1992 - 1993. The FNABs 10 features of the FNABs was marked as present or non present. These features along with the patients's age defines the attributes of the dataset. In addition, the final outcome of benign disease or malignancy was confirmed by open biopsy where this result was available.

\subsection{MIAS database}

Mias database contain results from 119 data points with 5 features: Character of background tissue, Class of abnormality, X coordinate of centre of abnormality, Y coordinate of centre of abnormality, Approximate radius (in pixels). The features was extracted from 1024x1024 pixel images.

\subsection{Erlangen-Nuremberg}

Dataset collected from a Breast Imaging-Reporting and Data System (BI-RADS) at the Institute of Radiology of the University Erlangen-Nuremberg between 2003 and 2006. It contains three features assessed as a discrete value from a double-review by physicians along with the patients' age.

\input{snippets/table_datasets}


\section{Implementation}

The implementation outline is a straightforward approach. Each dataset will be split into training and test data. For each classifier each FS-method will select all possible subsets in turn. The classifier will be trained on the subset and evaluated on the test data. A compact pseudocode how results are produced is presented in algorithm \ref{alg:pseudo_code}. The steps is more more thoroughly detailed below. Measurements of how results will be evaluated is contained in \ref{Evaluation}.

\input{snippets/implementation_pseudo}

\subsection{Classifiers and parameters}
All classifiers was imported from Scikit \parencite{scikit-learn}. All classifiers allow tuning by setting parameters of its behaviour. As tuning the parameters for any dataset and/or subset of attributes inflicts bias to the current state all parameters was left to default. Default parameters may cause suboptimal performance of a classifier. An optimal is not the intention of this experiment, the influence of feature selection is. Thus motivating the default parameter settings. Those values of the most influential parameters for each classifier can be found in table \ref{table:classifier_params}.
% Move these setting to appendix as they don't contribute much here?


\subsection{Feature selection}

Feature selection with filtering methods was also imported from  Scikit \parencite{scikit-learn}. The library contains the method ``SelectKBest'' which tranforms data to a subset of $k$ attributes given a method such as Chi2 or Entropy.

Feature selection with wrapper methods was implemented with the ``SequentialFeatureSelector'' method available in the a library by \textcite{mlextend}. As the wrapper evaluates the performance of each subset when selecting the best, a measurement method of evaluations must be set. The method implemented was accuracy as that is what should be used to compare methods at a later stage.


\section{Evaluation}
\label{Evaluation}

\subsection{Test data and accuracy}

To compare methods and classifiers a measurement is needed. Classification accuracy entails how many labels was correct on a test set, $correct\_classified\_samples/all\_samples$. To ensure correct classification the data split is kept consistent between classifiers by seeding. Test data is only introduced when accuracy is measured to avoid data leakage. Test data is maximised by performing stratified 10-fold validation providing 10 accuracy scores on 10 distinct subsets of the test data. The overall performance is computed as the mean over the folds.

To evaluate the impact of using feature selection the gain will be computed. The gain is computed as the ratio between best accuracy using FS and accuracy not using FS. The accumulated gains of a classifier gives a measure of how much it improved from feature selection and a basis to compare classifiers against each other.

\subsection{Explaining differences between datasets}

It is needful to evaluate the difference between datasets. The differences among the datasets allows to study how effects of FS-methods generalises between different data and if results are consistent. To explain any inconsistencies analysis of variance (ANOVA) test will be performed \parencite{sthle1989}. ANOVA entails if differences in results between groups can be explained by variance or if there is a statistically significant difference between groups. The groups of interest are FS-methods, classifiers and datasets. ANOVA computations of groups and their interactions results in F-scores. F-score measures the probability of rejecting the null hypothesis, that some combination of groups are equal.
